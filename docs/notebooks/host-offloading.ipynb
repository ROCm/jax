{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQbS50fIdHw1"
   },
   "source": [
    "(host-offloading)=\n",
    "# Host Offloading\n",
    "\n",
    "<!--* freshness: { reviewed: '2025-04-10' } *-->\n",
    "\n",
    "This tutorial provides a practical introduction to host offloading techniques in JAX, focusing on:\n",
    "\n",
    "- Activation offloading\n",
    "- Parameter offloading\n",
    "\n",
    "By applying offloading strategies, you can better manage memory resources and reduce memory pressure on your devices. To implement these strategies effectively, you'll need to understand JAX's core mechanisms for data placement and movement.\n",
    "\n",
    "## Building Blocks for Offloading\n",
    "\n",
    "JAX provides several key components for controlling where and how data are stored and moved between the host and the device memory. In the following sections, you'll explore:\n",
    "\n",
    "- How to specify data distribution with sharding\n",
    "- How to control memory placement between host and device\n",
    "- How to manage data movement in jitted functions\n",
    "- How to control internal sharding within computations\n",
    "\n",
    "### NamedSharding and Memory Kinds\n",
    "\n",
    "{class}`~jax.sharding.NamedSharding` defines how data are distributed across devices. It includes:\n",
    "\n",
    "- Basic data distribution configuration\n",
    "- `memory_kind` parameter for specifying memory type (`device` or `pinned_host`)\n",
    "- By default, `memory_kind` is set to `device` memory\n",
    "- `with_memory_kind` method for creating new sharding with modified memory type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-6sxUlqrlBn",
    "outputId": "79e7fbda-de0e-4951-9949-77039b2fae81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NamedSharding(mesh=Mesh('x': 1, 'y': 1), spec=PartitionSpec('x', 'y'), memory_kind=device)\n",
      "NamedSharding(mesh=Mesh('x': 1, 'y': 1), spec=PartitionSpec('x', 'y'), memory_kind=pinned_host)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec as P\n",
    "import numpy as np\n",
    "\n",
    "# Create mesh\n",
    "# 1x1 mesh represents a single device with two named dimensions (x and y)\n",
    "mesh = Mesh(np.array(jax.devices()[0]).reshape(1,1), ('x','y'))\n",
    "\n",
    "# Device sharding - partitions data along x and y dimensions\n",
    "s_dev = NamedSharding(mesh, P('x', 'y'), memory_kind=\"device\")\n",
    "\n",
    "# Host sharding - same partitioning but in pinned host memory\n",
    "s_host = s_dev.with_memory_kind('pinned_host')\n",
    "\n",
    "print(s_dev)   # Shows device memory sharding\n",
    "print(s_host)  # Shows pinned host memory sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_pB9465VoMP"
   },
   "source": [
    "### Data Placement with device_put\n",
    "\n",
    "{func}`jax.device_put` is a function that explicitly transfers arrays to a specified memory location according to a sharding specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJFnf7FGp6Lj",
    "outputId": "a6c1fcdd-e49e-4017-c7aa-8be2e394c3a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinned_host\n",
      "device\n"
     ]
    }
   ],
   "source": [
    "# Create a 4x8 array\n",
    "arr = jnp.arange(32.0).reshape(4, 8)\n",
    "\n",
    "# Move arrays to different memory locations based on sharding objects\n",
    "arr_host = jax.device_put(arr, s_host)  # Places in pinned host memory\n",
    "arr_dev = jax.device_put(arr, s_dev)    # Places in device memory\n",
    "\n",
    "# Verify memory locations\n",
    "print(arr_host.sharding.memory_kind)  # Output: pinned_host\n",
    "print(arr_dev.sharding.memory_kind)   # Output: device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHXvBpQKTMCR"
   },
   "source": [
    "### Input/Output Sharding Controls\n",
    "\n",
    "Shardings determine how data are split across devices. JAX provides two key parameters for controlling data placement in jitted functions:\n",
    "1. `in_shardings`: controls how input arrays are partitioned when entering a jitted function\n",
    "2. `out_shardings`: controls how output arrays are partitioned when leaving a jitted function\n",
    "  - Can differ from input sharding\n",
    "  - Allows different memory kinds for outputs\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZXNj9NUeaIdX"
   },
   "outputs": [],
   "source": [
    "# Function with different input and output shardings\n",
    "def compute_function(x):\n",
    "  return x * 2\n",
    "\n",
    "compute_function = jax.jit(\n",
    "    compute_function,\n",
    "    in_shardings=s_host,  # Input arrays will be in host memory\n",
    "    out_shardings=s_dev   # Output arrays will be in device memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbE-eBrJTBuS"
   },
   "source": [
    "### Internal Sharding Control\n",
    "\n",
    "{func}`jax.lax.with_sharding_constraint` is a function that allows you to specify how an array should be sharded at a particular point within a JAX computation. It allows you:\n",
    "- Controls sharding within computations for intermediate values and outputs\n",
    "- Alternative to {func}`jax.device_put`\n",
    "- Works with {func}`jax.jit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LIP5A01fVcrY"
   },
   "outputs": [],
   "source": [
    "from jax.lax import with_sharding_constraint\n",
    "\n",
    "@jax.jit\n",
    "def func(x):\n",
    "  # Force x to be sharded across devices in a specific way\n",
    "  x = with_sharding_constraint(x, P('x'))\n",
    "  return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nn8I7weaz8r"
   },
   "source": [
    "With these sharding and memory placement techniques, you can apply them flexibly according to your needs in the offloading strategies. The combination of:\n",
    "- {class}`~jax.sharding.NamedSharding` for data distribution\n",
    "- `memory_kind` and `with_memory_kind` for memory type control\n",
    "- {func}`jax.device_put` for explicit data placement\n",
    "- `in_shardings` and `out_shardings` for input/output data placement in jitted functions\n",
    "- {func}`jax.lax.with_sharding_constraint` for internal sharding control\n",
    "\n",
    "provides a comprehensive toolkit for managing data placement and movement between host and device memory, enabling efficient implementation of various offloading patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhLVvRO2p6Lj"
   },
   "source": [
    "## Activation Offloading\n",
    "\n",
    "The detailed coverage of activation offloading can be found in the {ref}`gradient-checkpointing` tutorial. Activation offloading helps manage memory by moving intermediate activations to host memory after the forward pass, and bringing them back to device memory during the backward pass when needed for gradient computation.\n",
    "\n",
    "To implement activation offloading effectively, you need to understand checkpoint names and policies. Here's how they work in a simple example:\n",
    "\n",
    "### Checkpoint Names\n",
    "\n",
    "The {func}`checkpoint_name` function allows you to label activations for memory management during computation. Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sLO9ceS6p6Lj"
   },
   "outputs": [],
   "source": [
    "from jax.ad_checkpoint import checkpoint_name\n",
    "\n",
    "def layer(x, w):\n",
    "  w1, w2 = w\n",
    "  x = checkpoint_name(x, \"x\")\n",
    "  y = x @ w1\n",
    "  return y @ w2, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_T92oCOp6Lk"
   },
   "source": [
    "This example shows:\n",
    "\n",
    "* A simple neural network layer with two matrix multiplications\n",
    "* Labeling of input activation x with identifier `\"x\"`\n",
    "* Sequential operations:\n",
    "  1. First multiplication: `x @ w1`\n",
    "  2. Second multiplication: `y @ w2`\n",
    "\n",
    "The checkpoint name helps the system decide whether to:\n",
    "* Keep the activation in device memory or\n",
    "* Offload it to host memory during computation\n",
    "\n",
    "This pattern is common in neural networks, where multiple transformations are applied sequentially to input data.\n",
    "\n",
    "\n",
    "### Checkpoint Policies\n",
    "\n",
    "The {func}`jax.remat` transformation manages memory by handling intermediate values through three strategies:\n",
    "\n",
    "1. Recomputing during backward pass (default behavior)\n",
    "2. Storing on device\n",
    "3. Offloading to host memory after forward pass and loading back during backward pass\n",
    "\n",
    "Example of setting an offloading checkpoint policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "W8Usw_wOp6Lk"
   },
   "outputs": [],
   "source": [
    "from jax import checkpoint_policies as cp\n",
    "\n",
    "policy = cp.save_and_offload_only_these_names(\n",
    "    names_which_can_be_saved=[],          # No values stored on device\n",
    "    names_which_can_be_offloaded=[\"x\"],   # Offload activations labeled \"x\"\n",
    "    offload_src=\"device\",                 # Move from device memory\n",
    "    offload_dst=\"pinned_host\"             # To pinned host memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0XslpYzp6Lk"
   },
   "source": [
    "Since {func}`jax.lax.scan` is commonly used in JAX for handling sequential operations (like RNNs or transformers), you need to know how to apply your offloading strategy in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xCrxjTx_p6Lk"
   },
   "outputs": [],
   "source": [
    "def scanned(w, x):\n",
    "  remat_layer = jax.remat(layer,\n",
    "                          policy=policy,     # Use our offloading policy\n",
    "                          prevent_cse=False) # Allow CSE optimizations\n",
    "  result = jax.lax.scan(remat_layer, x, w)[0]\n",
    "  return jnp.sum(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UasMfG8Sp6Lk"
   },
   "source": [
    "Key components:\n",
    "\n",
    "* {func}`jax.remat` applies our checkpoint policy to the layer function\n",
    "* `prevent_cse=False` enables XLA's common subexpression elimination for better performance\n",
    "* {func}`jax.lax.scan` iterates the rematerialized layer along an axis\n",
    "\n",
    "### Example Execution\n",
    "\n",
    "Here's how the code initializes and executes the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "y_xX3eb7p6Lk"
   },
   "outputs": [],
   "source": [
    "# Initialize input and weights with small values (0.0001)\n",
    "input = jnp.ones((256, 256), dtype=jnp.float32) * 0.0001  # Input matrix: 256 x 256\n",
    "w1 = jnp.ones((10, 256, 1024), dtype=jnp.float32) * 0.0001 # 10 layers of 256 x 1024 matrices\n",
    "w2 = jnp.ones((10, 1024, 256), dtype=jnp.float32) * 0.0001 # 10 layers of 1024 x 256 matrices\n",
    "\n",
    "# Compile and compute gradients of the scanned function\n",
    "f = jax.jit(jax.grad(scanned))  # Apply JIT compilation to gradient computation\n",
    "result_activation = f((w1, w2), input)     # Execute the function with weights and input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tx7aara42pY"
   },
   "source": [
    "### Summary of Activation Offloading\n",
    "\n",
    "Activation offloading provides a powerful way to manage memory in large computations by:\n",
    "\n",
    "* Using checkpoint names to mark specific activations\n",
    "* Applying policies to control where and how activations are stored\n",
    "* Supporting common JAX patterns like scan operations\n",
    "* Moving selected activations to host memory when device memory is under budget\n",
    "\n",
    "This approach is particularly useful when working with large models that would otherwise exceed device memory capacity.\n",
    "\n",
    "## Parameter Offloading\n",
    "\n",
    "Model parameters (also known as weights) can be offloaded to the host memory to optimize device memory usage during initialization. This is achieved by using {func}`jax.jit` with a sharding strategy that specifies host memory kind.\n",
    "\n",
    "While parameter offloading and activation offloading are distinct memory optimization techniques, the following example demonstrates parameter offloading built upon the activation offloading implementation shown earlier.\n",
    "\n",
    "### Parameter Placement for Computation\n",
    "\n",
    "Different from the earlier `layer` function, {func}`jax.device_put` is applied to move parameter `w1` and `w2` to the device before the  matrix multiplications. This ensures the parameters are available on the device for both forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1qGN2hBQdheo"
   },
   "outputs": [],
   "source": [
    "# Hybrid version: Both activation and parameter offloading\n",
    "def hybrid_layer(x, w):\n",
    "  # Move model parameters w1 and w2 to host memory via device_put\n",
    "  w1, w2 = jax.tree.map(lambda x: jax.device_put(x, s_dev), w)\n",
    "  x = checkpoint_name(x, \"x\")  # Offload activation x to host memory\n",
    "  y = x @ w1\n",
    "  return y @ w2, None\n",
    "\n",
    "def hybrid_scanned(w, x):\n",
    "  remat_layer = jax.remat(hybrid_layer,     # Use hybrid_layer instead of layer\n",
    "                          policy=policy,     # Use offloading policy\n",
    "                          prevent_cse=False) # Allow CSE optimizations\n",
    "  result = jax.lax.scan(remat_layer, x, w)[0]\n",
    "  return jnp.sum(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcgpNztNp6Lk"
   },
   "source": [
    "Note that the activation offloading implementation remains unchanged, using the same:\n",
    "* Checkpoint name `\"x\"`\n",
    "* Checkpoint policy\n",
    "* `scanned` function combining {func}`jax.remat` and {func}`jax.lax.scan`\n",
    "\n",
    "### Parameter Initialization with Host Offloading\n",
    "\n",
    "During the initialization, parameter `w1` and `w2` are placed on host memory before being passed to the {func}`jax.jit` function `f`. Note that {func}`jax.device_put` is used here instead of `in_shardings` because:\n",
    "- `in_shardings` would need to be specified in the {func}`jax.jit` decoration, affecting all inputs (both `(w1, w2)` and `input`).\n",
    "- Using {func}`jax.device_put` outside the jitted function allows us to selectively place only the parameters on host memory while keeping the `input` variable on device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHEoG9qGp6Lk",
    "outputId": "7290e342-f0f1-4c85-8155-8fc374f88f47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match within tolerance: True\n"
     ]
    }
   ],
   "source": [
    "# Move model parameters w1 and w2 to the host via device_put\n",
    "# Initialize input and weights with small values (0.0001)\n",
    "wh1 = jax.device_put(w1, s_host)\n",
    "wh2 = jax.device_put(w2, s_host)\n",
    "\n",
    "# Compile and compute gradients of the scanned function\n",
    "f = jax.jit(jax.grad(hybrid_scanned))  # Apply JIT compilation to gradient computation\n",
    "result_both = f((wh1, wh2), input) # Execute with both activation and parameter offloading\n",
    "\n",
    "# Verify numerical correctness\n",
    "are_close = jnp.allclose(\n",
    "    result_activation[0],    # Result from activation offloading only\n",
    "    result_both[0],         # Result from both activation and parameter offloading\n",
    "    rtol=1e-5,\n",
    "    atol=1e-5\n",
    ")\n",
    "print(f\"Results match within tolerance: {are_close}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVpozzwHflQk"
   },
   "source": [
    "The matching results verify that initializing parameters on host memory maintains computational correctness.\n",
    "\n",
    "### Limitation of Parameter Offloading\n",
    "\n",
    "{func}`jax.lax.scan` is crucial for effective parameter management. Using an explicit for loop would cause parameters to continuously occupy device memory, resulting in the same memory usage as without parameter offloading. While {func}`jax.lax.scan` allows specifying the scan axis, parameter offloading currently works only when scanning over axis 0. Scanning over other axes generates a `transpose` operation during compilation before returning parameters to the device, which is expensive and not supported on all platforms.\n",
    "\n",
    "## Tools for Host Offloading\n",
    "\n",
    "For device memory analysis, refer to :doc:`device_memory_profiling`. The profiling tools described in {ref}`profiling` can help measure memory savings and performance impact from host offloading."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
